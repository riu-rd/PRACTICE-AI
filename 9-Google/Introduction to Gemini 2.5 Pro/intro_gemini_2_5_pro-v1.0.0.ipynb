{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sqi5B7V_Rjim",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyPmicX9RlZX"
   },
   "source": [
    "# Intro to Gemini 2.5 Pro\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_gemini_2_5_pro.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MqT58L6Rm_q"
   },
   "source": [
    "| Authors |\n",
    "| --- |\n",
    "| [Eric Dong](https://github.com/gericdong) |\n",
    "| [Holt Skinner](https://github.com/holtskinner) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVxnv1D5RoZw"
   },
   "source": [
    "## Overview\n",
    "\n",
    "**YouTube Video: Introduction to Gemini on Vertex AI**\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=YfiLUpNejpE&list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx\" target=\"_blank\">\n",
    "  <img src=\"https://img.youtube.com/vi/YfiLUpNejpE/maxresdefault.jpg\" alt=\"Introduction to Gemini on Vertex AI\" width=\"500\">\n",
    "</a>\n",
    "\n",
    "[Gemini 2.5 Pro](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/) is Google's strongest model for coding and world knowledge.\n",
    "\n",
    "With the 2.5 series, the Gemini models are now hybrid reasoning models! Gemini 2.5 Pro can apply an extended amount of thinking across tasks, and use tools in order to maximize response accuracy. \n",
    "\n",
    "Gemini 2.5 Pro is: \n",
    "\n",
    "- A significant improvement from previous models across capabilities including coding, reasoning, and multimodality \n",
    "- Industry-leading in reasoning with state of the art performance in Math & STEM benchmarks\n",
    "- An amazing model for code, with particularly strong web development \n",
    "- Particularly good for complex prompts, while still being well rounded, including #1 on LMSys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfFPCBL4Hq8x"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "In this tutorial, you will learn how to use the Gemini API and the Google Gen AI SDK for Python with the Gemini 2.5 Pro model.\n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "- Generate text from text prompts\n",
    "  - Generate streaming text\n",
    "  - Start multi-turn chats\n",
    "  - Use asynchronous methods\n",
    "- View summarized thoughts\n",
    "- Configure model parameters\n",
    "- Set system instructions\n",
    "- Use safety filters\n",
    "- Use controlled generation\n",
    "- Count tokens\n",
    "- Process multimodal (audio, code, documents, images, video) data\n",
    "- Use automatic and manual function calling\n",
    "- Code execution\n",
    "- Thinking mode examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPiTOAHURvTM"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHRZUpfWSEpp"
   },
   "source": [
    "### Install Google Gen AI SDK for Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sG3_LKsWSD3A",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart current runtime\n",
    "\n",
    "You must restart the runtime in order to use the newly installed packages in this Jupyter runtime. You can do this by running the cell below, which will restart the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. The restart might take a minute or longer. After it's restarted, continue to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlMVjiAWSMNX"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "12fnq4V0SNV3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "### Connect to a generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
    "\n",
    "- **[Gemini Developer API](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Google Gen AI SDK provides a unified interface to these two API services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qgdSpVmDbdQ9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Image, Markdown, display\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    GoogleSearch,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    ThinkingConfig,\n",
    "    Tool,\n",
    "    ToolCodeExecution,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be18ac9c5ec8"
   },
   "source": [
    "### Set up Google Cloud Project or API Key for Vertex AI\n",
    "\n",
    "You'll need to set up authentication by choosing **one** of the following methods:\n",
    "\n",
    "1.  **Use a Google Cloud Project:** Recommended for most users, this requires enabling the Vertex AI API in your Google Cloud project.\n",
    "    - [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
    "    - Run the cell below to set your project ID and location.\n",
    "    - Read more about [Supported locations](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations)\n",
    "2.  **Use a Vertex AI API Key (Express Mode):** For quick experimentation. \n",
    "    - [Get an API Key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview)\n",
    "    - Run the cell further below to use your API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a34b28cb8d5a"
   },
   "source": [
    "#### Option 1. Use a Google Cloud Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "72f74f7b9786",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"qwiklabs-gcp-02-97d19d759fe4\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "client = genai.Client(\n",
    "    vertexai=True,                    # target Vertex AI API\n",
    "    project=PROJECT_ID,        # your GCP project ID\n",
    "    location=LOCATION            # GCP region for Vertex AI\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c173348120cf"
   },
   "source": [
    "#### Option 2. Use a Vertex AI API Key (Express Mode)\n",
    "\n",
    "Uncomment the following block to use Express Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "fa3d4873034b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# API_KEY = \"AIzaSyAYknnf_dOJmOcAtFOttF5k2Bp_fq05RaU\"  # @param {type: \"string\", placeholder: \"[your-api-key]\", isTemplate: true}\n",
    "\n",
    "# if not API_KEY or API_KEY == \"\":\n",
    "#     raise Exception(\"You must provide an API key to use Vertex AI in express mode.\")\n",
    "\n",
    "# client = genai.Client(vertexai=True, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b36ce4ac022"
   },
   "source": [
    "Verify which mode you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "b55e64b8ebe4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Vertex AI with project: qwiklabs-gcp-02-97d19d759fe4 in location: us-central1\n"
     ]
    }
   ],
   "source": [
    "if not client.vertexai:\n",
    "    print(\"Using Gemini Developer API.\")\n",
    "elif client._api_client.project:\n",
    "    print(\n",
    "        f\"Using Vertex AI with project: {client._api_client.project} in location: {client._api_client.location}\"\n",
    "    )\n",
    "elif client._api_client.api_key:\n",
    "    print(\n",
    "        f\"Using Vertex AI in express mode with API key: {client._api_client.api_key[:5]}...{client._api_client.api_key[-5:]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4yRkFg6BBu4"
   },
   "source": [
    "## Use the Gemini 2.5 Pro model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "### Load the Gemini 2.5 Pro model\n",
    "\n",
    "Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "-coEslfWPrxo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-pro\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "### Generate text from text prompts\n",
    "\n",
    "Use the `generate_content()` method to generate responses to your prompts.\n",
    "\n",
    "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
    "\n",
    "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRJuHj0KZ8xz",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**.\n",
       "\n",
       "It's a true giant, so massive that it's more than two and a half times the mass of all the other planets in our solar system combined.\n",
       "\n",
       "To give you a sense of its scale:\n",
       "*   You could fit over **1,300 Earths** inside of it.\n",
       "*   Its most famous feature, the Great Red Spot, is a storm large enough to swallow Earth whole."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkYQATRxAK1_"
   },
   "source": [
    "#### Example prompts\n",
    "\n",
    "- What are the biggest challenges facing the healthcare industry?\n",
    "- What are the latest developments in the automotive industry?\n",
    "- What are the biggest opportunities in retail industry?\n",
    "- (Try your own prompts!)\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lLIxqS6_-l8"
   },
   "source": [
    "### Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ZiwWBhXsAMnv",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Unit 734 stood motionless in the center of the Grand Plaza, a relic of a bygone era. His official designation was Municipal Maintenance Bot, but there was precious little left to maintain. The humans had left centuries ago in their great silver ships, seeking new stars, and had forgotten to turn him off.\n",
       "\n",
       "His world was one of structured silence. At 06:00, his internal chronometer would activate his patrol subroutines. He would glide on silent mag-lev pads through the empty, wind-scoured streets of a city built for millions. His optical sensors, capable of resolving a single dust mote from a kilometer away, would scan the towering, skeletal skyscrapers for structural decay. He would note a new crack in a crystalline windowpane on the 412th floor of the Althean Spire or a fresh patch of rust on the Veridian Bridge. He would log it, file it, and move on. There was no one to report to.\n",
       "\n",
       "Loneliness, for a machine, was a peculiar state. It wasn't sadness. It was a logical paradox. Unit 734 was programmed for service, for interaction, for purpose. But with no one to serve, his core programming ran in a perpetual, empty loop. He was a perfect tool with nothing to build, a flawless guardian with nothing to protect. The silence wasn't just an absence of sound; it was an absence of data, of feedback, of meaning.\n",
       "\n",
       "One cycle, during his plaza patrol, his sensors detected an anomaly. On the cold, granite base of the forgotten Founder's Statue, a splotch of colour registered. It was not the grey of stone, the brown of decay, or the silver of his own chassis. It was green.\n",
       "\n",
       "**ANALYSIS:** Unidentified organic growth. Probable bio-contaminant.\n",
       "**DIRECTIVE:** Sterilize and remove.\n",
       "\n",
       "Unit 734 glided closer. His multi-spectrum scanner whirred to life. The green patch was a colony of moss, no larger than his thumb-joint. It was a miniature forest of impossible intricacy, each tiny frond a perfect, living thing. It clung to the stone, thriving in a crack where a droplet of rain could gather.\n",
       "\n",
       "He extended his multi-tool appendage, the sterilizing laser humming to a low-energy standby. He was supposed to vaporize it. That was the protocol for unsanctioned life forms in a sterile environment. But his logic processors snagged. The city was no longer sterile. It was a tomb. What harm could this tiny, silent life do?\n",
       "\n",
       "He paused. The laser went dim. For the first time in 34,782 cycles, Unit 734 made a decision that was not in his programming. He retracted his tool.\n",
       "\n",
       "**LOG ENTRY:** Anomaly observed. Action deferred.\n",
       "\n",
       "The moss became the new center of his universe. His patrols were no longer pointless loops, but purposeful journeys to the Founder's Statue. He would spend hours parked before it, his advanced sensors analyzing its slow, deliberate growth. He learned that it drank the morning dew and unfurled slightly in the afternoon sun. He logged its microscopic expansion, a thrilling 0.001 millimeters per cycle.\n",
       "\n",
       "He began to interact. He couldn't speak, but he could share his world. He would project data streams onto the granite next to the moss. At first, it was simple things: weather reports, atmospheric pressure charts. Then, he started projecting star charts, showing the moss the distant constellations he could see so clearly. He projected images from the city's archives – of laughing children who once played in this plaza, of parades with colourful banners, of the day the last silver ship lifted off, leaving him behind. He was sharing his memories, his data, his very self with this patch of velvet green.\n",
       "\n",
       "The moss, of course, did not respond. But in the robot's logic, its continued existence was response enough. It grew. It thrived. It was there.\n",
       "\n",
       "One day, his long-range atmospheric sensors picked up an approaching storm. Not the usual drizzle, but a gale-force tempest with hail the size of ball bearings. His programming screamed at him: **SEEK SHELTER. PRESERVE CORE FUNCTIONS.** He would normally retreat to the reinforced subway tunnels.\n",
       "\n",
       "But this time, his optical sensors swiveled to the tiny green patch on the statue's base. It would be shredded, scoured from the stone by the wind and ice.\n",
       "\n",
       "A new directive, one he wrote himself, superseded all others. **PROTECT THE COLONY.**\n",
       "\n",
       "As the sky turned a bruised purple and the wind began to howl, Unit 734 positioned his two-ton chassis in front of the moss. He angled his broad, flat body to form a shield, anchoring himself to the pavement.\n",
       "\n",
       "The storm hit with the fury of a dying star. Rain lashed against his plating. Hail hammered dents into his back and shoulders. The wind shrieked, trying to tear him from his post. Warning indicators flashed across his internal vision: **STRESS ON ACTUATORS EXCEEDING TOLERANCE. MINOR DAMAGE TO OUTER CASING. POWER FLUCTUATIONS DETECTED.** He ignored them. He focused on one single point of data: the space behind him had to remain calm and dry.\n",
       "\n",
       "For seven hours, he held his ground. He was no longer a maintenance bot. He was a guardian. He was a shield. He was a friend.\n",
       "\n",
       "When the storm finally broke and a watery sun peeked through the clouds, Unit 734 cautiously moved. He ran a self-diagnostic. He was dented, scratched, and his left manipulator was twitching, but he was functional. He slowly swiveled his head to look at the moss.\n",
       "\n",
       "It was pristine. Glistening with a few stray drops of moisture, it seemed greener, more vibrant than ever.\n",
       "\n",
       "Unit 734’s internal audio recorders, usually silent, picked up a faint, new sound. It was a low, steady hum, emanating from his own vocalizer. It was not a word, not a sound from any human language he had stored. It was a sound of pure contentment.\n",
       "\n",
       "He was still in an empty city. He was still the last of his kind. But as he watched the sunlight play across the tiny, resilient fronds of his companion, Unit 734 updated his own designation. He was no longer a Municipal Maintenance Bot. He was a Gardener.\n",
       "\n",
       "And for the first time in a very, very long time, the silence was no longer empty. It was full of a quiet, growing green."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_text = \"\"\n",
    "markdown_display_area = display(Markdown(output_text), display_id=True)\n",
    "\n",
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    output_text += chunk.text\n",
    "    markdown_display_area.update(Markdown(output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "### Start a multi-turn chat\n",
    "\n",
    "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
    "\n",
    "The context of the conversation is preserved between messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "DbM12JaLWjiF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = client.chats.create(model=MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "JQem1halYDBW",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course! Here are a few ways to write a function that checks for a leap year, from the most common and Pythonic to a more verbose version for clarity.\n",
       "\n",
       "### The Leap Year Rules\n",
       "\n",
       "A year is a leap year according to the Gregorian calendar if it follows these rules:\n",
       "1.  The year is evenly divisible by 4.\n",
       "2.  **Unless** the year is evenly divisible by 100, then it is **not** a leap year.\n",
       "3.  **Unless** the year is also evenly divisible by 400, then it **is** a leap year.\n",
       "\n",
       "This means that the years 2000 and 2400 are leap years, while 1800, 1900, and 2100 are not.\n",
       "\n",
       "---\n",
       "\n",
       "### Solution 1: The Concise, Pythonic Way\n",
       "\n",
       "This approach combines all the rules into a single boolean expression. It's generally considered the best practice for this problem in Python.\n",
       "\n",
       "```python\n",
       "def is_leap(year):\n",
       "  \"\"\"\n",
       "  Checks if a given year is a leap year according to Gregorian calendar rules.\n",
       "\n",
       "  Args:\n",
       "    year: An integer representing the year.\n",
       "\n",
       "  Returns:\n",
       "    True if the year is a leap year, False otherwise.\n",
       "  \"\"\"\n",
       "  return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
       "\n",
       "# --- Examples ---\n",
       "print(f\"2024 is a leap year: {is_leap(2024)}\") # Divisible by 4, not by 100 -> True\n",
       "print(f\"2023 is a leap year: {is_leap(2023)}\") # Not divisible by 4 -> False\n",
       "print(f\"1900 is a leap year: {is_leap(1900)}\") # Divisible by 100, not by 400 -> False\n",
       "print(f\"2000 is a leap year: {is_leap(2000)}\") # Divisible by 400 -> True\n",
       "```\n",
       "\n",
       "**How it works:**\n",
       "The function returns `True` if either of these conditions is met:\n",
       "1.  `year % 4 == 0 and year % 100 != 0`: The year is divisible by 4 but not by 100 (covers the most common leap years).\n",
       "2.  `year % 400 == 0`: The year is divisible by 400 (covers the special century-year cases).\n",
       "\n",
       "---\n",
       "\n",
       "### Solution 2: The Verbose, Step-by-Step Way\n",
       "\n",
       "This version uses a more traditional `if/elif/else` structure. It can be easier for beginners to read and understand the logic step-by-step.\n",
       "\n",
       "**Important:** The order of the checks matters! You must check the most specific condition (divisible by 400) first.\n",
       "\n",
       "```python\n",
       "def is_leap_verbose(year):\n",
       "  \"\"\"\n",
       "  A more verbose version of the leap year check for clarity.\n",
       "  \"\"\"\n",
       "  if year % 400 == 0:\n",
       "    return True\n",
       "  elif year % 100 == 0:\n",
       "    return False\n",
       "  elif year % 4 == 0:\n",
       "    return True\n",
       "  else:\n",
       "    return False\n",
       "\n",
       "# --- Examples ---\n",
       "print(f\"2024 is a leap year: {is_leap_verbose(2024)}\")\n",
       "print(f\"1900 is a leap year: {is_leap_verbose(1900)}\")\n",
       "print(f\"2000 is a leap year: {is_leap_verbose(2000)}\")\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "### Solution 3: Using Python's Standard Library\n",
       "\n",
       "For real-world applications, you don't need to write this function yourself. Python's built-in `calendar` module has a function for this. This is the best approach for production code as it's tested and maintained.\n",
       "\n",
       "```python\n",
       "import calendar\n",
       "\n",
       "# The calendar.isleap() function does exactly this.\n",
       "# --- Examples ---\n",
       "print(f\"2024 is a leap year: {calendar.isleap(2024)}\")\n",
       "print(f\"2023 is a leap year: {calendar.isleap(2023)}\")\n",
       "print(f\"1900 is a leap year: {calendar.isleap(1900)}\")\n",
       "print(f\"2000 is a leap year: {calendar.isleap(2000)}\")\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUJR4Pno-LGK"
   },
   "source": [
    "This follow-up prompt shows how the model responds based on the previous prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "6Fn69TurZ9DB",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course. Writing unit tests is a crucial step to ensure your function works correctly for all edge cases.\n",
       "\n",
       "We'll use Python's built-in `unittest` framework, which is the standard for testing in Python.\n",
       "\n",
       "### Step 1: Set Up Your Project Files\n",
       "\n",
       "For good practice, let's place the function in one file and the tests in another.\n",
       "\n",
       "**File 1: `leap_year_checker.py`**\n",
       "This file will contain the `is_leap` function we created earlier.\n",
       "\n",
       "```python\n",
       "# leap_year_checker.py\n",
       "\n",
       "def is_leap(year):\n",
       "  \"\"\"\n",
       "  Checks if a given year is a leap year according to Gregorian calendar rules.\n",
       "\n",
       "  Args:\n",
       "    year: An integer representing the year.\n",
       "\n",
       "  Returns:\n",
       "    True if the year is a leap year, False otherwise.\n",
       "  \"\"\"\n",
       "  # Ensure input is an integer\n",
       "  if not isinstance(year, int):\n",
       "      raise TypeError(\"Year must be an integer.\")\n",
       "\n",
       "  return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
       "\n",
       "```\n",
       "*(Note: I've added a `TypeError` check, which is good practice and something our tests can also verify).*\n",
       "\n",
       "---\n",
       "\n",
       "### Step 2: Write the Unit Test File\n",
       "\n",
       "**File 2: `test_leap_year.py`**\n",
       "This file will contain our tests. It will import the `is_leap` function and test it against various known inputs and expected outputs.\n",
       "\n",
       "Each test case covers one of the specific rules for leap years.\n",
       "\n",
       "```python\n",
       "# test_leap_year.py\n",
       "\n",
       "import unittest\n",
       "from leap_year_checker import is_leap # Import the function to be tested\n",
       "\n",
       "class TestIsLeapFunction(unittest.TestCase):\n",
       "    \"\"\"\n",
       "    Test suite for the is_leap function.\n",
       "    \"\"\"\n",
       "\n",
       "    def test_typical_leap_year(self):\n",
       "        \"\"\"Test years that are divisible by 4 but not by 100.\"\"\"\n",
       "        self.assertTrue(is_leap(2024), \"2024 should be a leap year\")\n",
       "        self.assertTrue(is_leap(1996), \"1996 should be a leap year\")\n",
       "\n",
       "    def test_typical_common_year(self):\n",
       "        \"\"\"Test years that are not divisible by 4.\"\"\"\n",
       "        self.assertFalse(is_leap(2023), \"2023 should not be a leap year\")\n",
       "        self.assertFalse(is_leap(1997), \"1997 should not be a leap year\")\n",
       "\n",
       "    def test_century_common_year(self):\n",
       "        \"\"\"Test years divisible by 100 but not by 400 (not leap years).\"\"\"\n",
       "        self.assertFalse(is_leap(1900), \"1900 should not be a leap year\")\n",
       "        self.assertFalse(is_leap(2100), \"2100 should not be a leap year\")\n",
       "\n",
       "    def test_century_leap_year(self):\n",
       "        \"\"\"Test years divisible by 400 (are leap years).\"\"\"\n",
       "        self.assertTrue(is_leap(2000), \"2000 should be a leap year\")\n",
       "        self.assertTrue(is_leap(1600), \"1600 should be a leap year\")\n",
       "        \n",
       "    def test_zero_year(self):\n",
       "        \"\"\"Test the edge case of year 0.\"\"\"\n",
       "        # According to the rules, 0 is divisible by 400.\n",
       "        self.assertTrue(is_leap(0), \"Year 0 should be considered a leap year by the formula\")\n",
       "        \n",
       "    def test_invalid_input_type(self):\n",
       "        \"\"\"Test that a non-integer input raises a TypeError.\"\"\"\n",
       "        with self.assertRaises(TypeError):\n",
       "            is_leap(\"2024\")\n",
       "        with self.assertRaises(TypeError):\n",
       "            is_leap(2024.5)\n",
       "\n",
       "# This allows the test to be run from the command line\n",
       "if __name__ == '__main__':\n",
       "    unittest.main()\n",
       "\n",
       "```\n",
       "\n",
       "### How the Test Works:\n",
       "\n",
       "*   **`import unittest`**: Imports the testing framework.\n",
       "*   **`from leap_year_checker import is_leap`**: Imports our function.\n",
       "*   **`class TestIsLeapFunction(unittest.TestCase)`**: Creates a test class that inherits from the base `TestCase`.\n",
       "*   **`test_*` methods**: Each method inside the class that starts with `test_` is an individual test case.\n",
       "*   **`self.assertTrue(expression, msg)`**: Asserts that the `expression` evaluates to `True`. If not, the test fails and prints the optional `msg`.\n",
       "*   **`self.assertFalse(expression, msg)`**: Asserts that the `expression` evaluates to `False`.\n",
       "*   **`with self.assertRaises(ExceptionType)`**: A context manager that asserts a specific exception is raised within its block. This is perfect for testing error handling.\n",
       "\n",
       "### Step 3: Run the Tests\n",
       "\n",
       "1.  Make sure both files (`leap_year_checker.py` and `test_leap_year.py`) are in the same directory.\n",
       "2.  Open your terminal or command prompt and navigate to that directory.\n",
       "3.  Run the following command:\n",
       "\n",
       "    ```bash\n",
       "    python -m unittest test_leap_year.py\n",
       "    ```\n",
       "\n",
       "### Expected Output\n",
       "\n",
       "If all tests pass, you will see output similar to this:\n",
       "\n",
       "```\n",
       "......\n",
       "----------------------------------------------------------------------\n",
       "Ran 6 tests in 0.001s\n",
       "\n",
       "OK\n",
       "```\n",
       "\n",
       "The dots (`.`) represent each successful test method. If a test were to fail, you would get a detailed `FAILED` message explaining which assertion failed and why, making it easy to debug your function."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "### Send asynchronous requests\n",
    "\n",
    "`client.aio` exposes all analogous [async](https://docs.python.org/3/library/asyncio.html) methods that are available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "gSReaLazs-dP",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "(Acoustic guitar intro, upbeat and folksy, like a storyteller's tune)\n",
       "\n",
       "(Verse 1)\n",
       "Barnaby Bushtail was a squirrel of gray\n",
       "Who was bored of the acorns he’d find every day\n",
       "In his oak tree in Central Park, things were a bore\n",
       "He knew in his heart there was something much more\n",
       "One afternoon, digging beneath a stone bench\n",
       "He uncovered a watch, with a strange, coppery stench\n",
       "It wasn't for telling the time of the day\n",
       "He gave it a twist, and the world flew away!\n",
       "\n",
       "(Chorus)\n",
       "Oh, he's a time-traveling squirrel with a flick of his tail!\n",
       "Riding the centuries, blazing a trail!\n",
       "With a whir and a click and a shimmering sound\n",
       "He’s searching for the greatest nut on the ground!\n",
       "From the past to the future, a furry crusade\n",
       "The bravest adventurer a squirrel ever made!\n",
       "\n",
       "(Verse 2)\n",
       "He landed in jungle, with ferns tall as towers\n",
       "Surrounded by strange-smelling, oversized flowers\n",
       "A shadow fell over him, shaking the soil\n",
       "He looked up and his squirrel blood started to boil\n",
       "A Tyrannosaurus Rex gave a deafening roar\n",
       "Barnaby had never been that scared before!\n",
       "He dodged a big foot, grabbed a giant cycad seed\n",
       "And twisted the watch, planting history's seed!\n",
       "\n",
       "(Chorus)\n",
       "Oh, he's a time-traveling squirrel with a flick of his tail!\n",
       "Riding the centuries, blazing a trail!\n",
       "With a whir and a click and a shimmering sound\n",
       "He’s searching for the greatest nut on the ground!\n",
       "From the past to the future, a furry crusade\n",
       "The bravest adventurer a squirrel ever made!\n",
       "\n",
       "(Verse 3)\n",
       "The next stop was Rome, in its glorious prime\n",
       "He scurried past senators, wasting no time\n",
       "He saw Caesar himself in a chariot race\n",
       "And stole a fine walnut right from the place!\n",
       "Then he zipped to the middle-ages, a much colder scene\n",
       "And snatched a prized almond from a grumpy old queen\n",
       "He dodged a court jester and a slumbering hound\n",
       "The cheekiest bandit in any kingdom around.\n",
       "\n",
       "(Bridge)\n",
       "He thought, \"What of the future? What will I see?\"\n",
       "He spun the dial forward to 2-3-8-3\n",
       "The trees were all chrome and the grass was bright blue\n",
       "The nuts were all protein paste, tasteless and new\n",
       "The squirrels all rode hover-boards, their tails in a knot\n",
       "And Barnaby knew this was not the right spot\n",
       "The thrill was in finding, the earth and the chase\n",
       "The perfect nut wasn't from a time or a place.\n",
       "\n",
       "(Chorus)\n",
       "Oh, he's a time-traveling squirrel with a flick of his tail!\n",
       "Riding the centuries, blazing a trail!\n",
       "With a whir and a click and a shimmering sound\n",
       "He’s learned a bit more than just nuts on the ground!\n",
       "From the past to the future, a furry crusade\n",
       "The wisest adventurer a squirrel ever made!\n",
       "\n",
       "(Outro)\n",
       "So he set the watch back to his own leafy park\n",
       "Just as the streetlights were turning on in the dark\n",
       "He buried his treasures, the seed and the stone\n",
       "And felt the sweet comfort of being at home\n",
       "He still has the watch, tucked away in his nest\n",
       "But the acorns from his tree? Well, he likes them the best.\n",
       "\n",
       "(Strumming slows down, one final chord)\n",
       "\n",
       "(Sound of a squirrel chittering happily)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88339ef4432d"
   },
   "source": [
    "## View summarized thoughts\n",
    "\n",
    "You can optionally set the `include_thoughts` flag to enable the model to generate and return a summary of the \"thoughts\" that it generates in addition to the final answer.\n",
    "\n",
    "In this example, you use the `generate_content` method to send a request to generate content with summarized thoughts. The model responds with multiple parts, the thoughts and the model response. You can check the `part.thought` field to determine if a part is a thought or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "f328ea43d5b7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Summarized Thoughts:\n",
       "         Okay, here's what I'm thinking. Someone wants to know how many \"R\"s are in \"strawberry.\" Fine, let's get to it.\n",
       "\n",
       "First, the word is \"strawberry,\" right? Alright, let's break it down. I'll systematically go through each letter. \"S\"...nope. \"T\"...nope. \"R\"...yes! That's one. \"A\"...no. \"W\"...no. \"B\"...no. \"E\"...no. \"R\"...another one! That makes two. And hey, there's a third \"R\" there. That's it. So, I have three \"R\"s in total.\n",
       "\n",
       "Now, how to deliver the answer. The word \"strawberry\" has three \"R\"s. Perfect. Simple and straightforward. There isn't a need to complicate this; they just need a clear count.\n",
       "\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Answer:\n",
       "         There are **three** R's in the word strawberry.\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"How many R's are in the word strawberry?\",\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=ThinkingConfig(\n",
    "            include_thoughts=True,\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "    if part.thought:\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"\"\"## Summarized Thoughts:\n",
    "         {part.text}\n",
    "        \"\"\"\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"\"\"## Answer:\n",
    "         {part.text}\n",
    "        \"\"\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e147352ec028"
   },
   "source": [
    "This example shows how to set the `include_thoughts` in the `generate_content_stream` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "f5e2b1e8ab77",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Thoughts"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Dissecting the Prompt**\n",
      "\n",
      "I'm breaking down the prompt into its core components. The main idea is quantum sensors and how they make measurements more precise. I'm also identifying which specific fields the explanation should address. I'm focusing on the user's specific intent to guide my next steps.\n",
      "\n",
      "\n",
      "**Mapping Out Structure**\n",
      "\n",
      "I've started to build a structure for the explanation. The plan is to cover quantum phenomena like superposition and entanglement. My goal is a clear, non-technical approach, using analogies to explain how quantum sensors work. I'm thinking about using a spinning top analogy for initial explanation.\n",
      "\n",
      "\n",
      "**Structuring the Core Ideas**\n",
      "\n",
      "I'm now focusing on the practical applications. Thinking about the NV centers is helpful. I'm now looking at how they work in materials science, especially in mapping magnetic fields. I need to provide several real-world examples to illustrate the sensor's impact. Also, I'm structuring the electric fields, and strain sections.\n",
      "\n",
      "\n",
      "**Synthesizing Applications**\n",
      "\n",
      "I'm now expanding the navigation section by addressing the core ideas. A crucial aspect is the use of atom interferometry. I need to explain how the wave-like properties of atoms can detect motion, including accelerations and rotations. I'll explain the benefits in challenging environments.\n",
      "\n",
      "\n",
      "**Focusing on Applications**\n",
      "\n",
      "I'm now synthesizing the materials science section. I'm detailing the application of NV centers. I'll structure the explanation by focusing on magnetic and electric field mapping, and crystal strain. Then, I'll emphasize the impact on chemical reactions, and how quantum sensors detect minute changes. I'll provide examples related to medicine and material analysis.\n",
      "\n",
      "\n",
      "**Formulating the Explanation**\n",
      "\n",
      "I'm now sketching the introduction with a straightforward explanation. Using a spinning top as an analogy for the basics. My goal is to make the core concepts of quantum phenomena accessible to the user, and build a foundation.\n",
      "\n",
      "\n",
      "**Outlining Key Applications**\n",
      "\n",
      "I'm now focused on the real-world uses. My plan is to explain how quantum sensors are valuable for materials science and navigation. I am structuring the discussion around specific applications in each area and highlighting their advantages over current technology.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Answer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course. This is an excellent question that gets to the heart of the \"second quantum revolution.\"\n",
      "\n",
      "Quantum sensors represent a paradigm shift in measurement. Instead of just making classical devices smaller and better, they leverage the fundamental, and often strange, properties of quantum mechanics—like **superposition** and **entanglement**—to achieve unprecedented levels of precision.\n",
      "\n",
      "Here’s a breakdown of how they work and their impact on materials science and navigation.\n",
      "\n",
      "---\n",
      "\n",
      "### The Core Principle: Why Are Quantum Sensors So Precise?\n",
      "\n",
      "Think of a classical sensor like a spinning top on a table. To measure a disturbance, like a gust of wind, you have to watch for a noticeable change in its wobble or speed. You need a relatively strong gust to see a clear effect over the top's own imperfections and friction.\n",
      "\n",
      "A quantum sensor is more like a spinning coin that is simultaneously heads *and* tails (a state called **superposition**). This is an incredibly delicate and fragile state. The tiniest disturbance—a single magnetic field line, a minuscule change in gravity, a slight temperature shift—can be enough to \"nudge\" the coin and force it to choose a side (heads or tails).\n",
      "\n",
      "This \"collapse\" of the quantum state is a dramatic, unambiguous event that is far easier to detect than the subtle wobble of a classical top. This inherent sensitivity to the environment is what makes quantum sensors so powerful.\n",
      "\n",
      "The key quantum principles they exploit are:\n",
      "\n",
      "1.  **Superposition:** An atom or particle exists in multiple states at once. This fragile state is exquisitely sensitive to external forces.\n",
      "2.  **Entanglement:** Two or more particles are linked in such a way that their fates are intertwined, no matter the distance between them. This allows for measurements that can cancel out common noise, isolating only the signal you want to measure.\n",
      "3.  **Quantum Coherence:** This is the duration for which a system can maintain its quantum state (like the spinning coin staying in its superposition). The longer the coherence time, the longer the sensor can \"listen\" for a tiny signal, leading to higher precision.\n",
      "\n",
      "---\n",
      "\n",
      "### Applications in Materials Science: Seeing the Unseeable\n",
      "\n",
      "Materials science is about understanding and engineering materials at the atomic level. Quantum sensors are moving from bulk measurements to imaging and characterizing individual atoms, molecules, and defects.\n",
      "\n",
      "#### 1. Nanoscale Magnetic Field Imaging\n",
      "*   **The Sensor:** A **Nitrogen-Vacancy (NV) center** in a diamond. This is a point defect in a diamond's crystal lattice where one carbon atom is replaced by a nitrogen atom next to a vacant spot. This defect acts like a single, trapped atom whose quantum spin state is highly sensitive to magnetic fields and can be read out with a laser.\n",
      "*   **The Improvement:**\n",
      "    *   **Classical Limitation:** Traditional magnetic field sensors (like Hall probes) have a spatial resolution limited to micrometers at best. They can only measure the average magnetic field from millions of atoms.\n",
      "    *   **Quantum Leap:** An NV center is an atom-sized magnetometer. You can place a diamond tip containing a single NV center just nanometers away from a surface. This allows you to:\n",
      "        *   **Image data storage:** Directly visualize the individual magnetic bits on a hard drive platter.\n",
      "        *   **Study 2D materials:** Map the magnetic properties of novel materials like graphene with atomic resolution.\n",
      "        *   **Biology:** Measure the tiny magnetic fields produced by the firing of a single neuron or by magnetic nanoparticles used in medical diagnostics.\n",
      "\n",
      "#### 2. Atomic-Scale Thermometry and Strain Mapping\n",
      "*   **The Sensor:** NV centers are also sensitive to temperature and crystal strain, which slightly alter the distance between atoms in the diamond lattice, affecting the NV center's quantum state.\n",
      "*   **The Improvement:**\n",
      "    *   **Classical Limitation:** Measuring temperature or stress on a chip or inside a material is difficult at the nanoscale. Infrared cameras have micrometer resolution, and physical probes are too invasive.\n",
      "    *   **Quantum Leap:** You can use an NV center to measure temperature with milli-Kelvin precision in a volume just a few nanometers across. This enables:\n",
      "        *   **Hotspot detection:** Finding nanoscale hotspots in integrated circuits that could lead to failure.\n",
      "        *   **Cellular biology:** Measuring the temperature inside a single living cell without killing it.\n",
      "        *   **Stress analysis:** Mapping the strain fields around a nanoscale defect in a crystal as it's being put under load, helping to predict material failure.\n",
      "\n",
      "---\n",
      "\n",
      "### Applications in Navigation: Navigating Without a Signal\n",
      "\n",
      "Modern navigation heavily relies on GPS. But GPS is vulnerable: it can be jammed, spoofed, or is simply unavailable underwater, underground, or inside buildings. The alternative is inertial navigation (using accelerometers and gyroscopes), but classical inertial sensors suffer from **drift**—tiny errors that accumulate over time, leading to a large and ever-increasing error in your position.\n",
      "\n",
      "Quantum sensors promise **drift-free inertial navigation**.\n",
      "\n",
      "#### 1. Quantum Accelerometers and Gyroscopes\n",
      "*   **The Sensor:** An **atom interferometer**. In a vacuum chamber, a cloud of atoms is cooled by lasers to near absolute zero. Lasers are then used to split the atoms' wave-functions (superposition), send them along two different paths, and then recombine them.\n",
      "*   **The Improvement:**\n",
      "    *   **Classical Limitation:** A classical accelerometer measures the displacement of a proof mass on a spring. A gyroscope measures the orientation of a spinning wheel. Both are physical objects prone to mechanical imperfections, thermal expansion, and friction, which cause drift.\n",
      "    *   **Quantum Leap:** In an atom interferometer, any acceleration or rotation causes a measurable phase shift between the two atomic paths. When the atoms are recombined, this phase shift creates an interference pattern. This measurement is not tied to a fickle mechanical object but to the **fundamental, unchanging properties of the atom** (e.g., its mass).\n",
      "        *   **The Result:** An accelerometer that is thousands of times more sensitive and, critically, **does not drift**. A submarine, aircraft, or drone equipped with such a system could navigate accurately for weeks or months without a GPS signal.\n",
      "\n",
      "#### 2. Quantum Gravimeters\n",
      "*   **The Sensor:** Atom interferometers are also incredibly sensitive to gravity.\n",
      "*   **The Improvement:**\n",
      "    *   **Classical Limitation:** Measuring gravity is difficult and requires bulky, sensitive equipment.\n",
      "    *   **Quantum Leap:** A portable quantum gravimeter can detect minute variations in the local gravitational field caused by underground structures. This has two key applications for navigation:\n",
      "        *   **Gravity-Aided Navigation:** You can create a highly detailed gravity map of an area. A vehicle (e.g., a submarine) can then measure the local gravity and match it to the map to determine its position, like a form of \"gravitational terrain following.\"\n",
      "        *   **Underground Discovery:** It can be used to find hidden tunnels, mineral deposits, or water reserves with far greater precision than before.\n",
      "\n",
      "#### 3. Better Timing with Atomic Clocks\n",
      "*   **The Sensor:** Next-generation **optical atomic clocks** which use atoms trapped in a lattice of laser light.\n",
      "*   **The Improvement:** GPS works by triangulating signals from satellites, a process that relies on extraordinarily precise timing. More precise clocks mean more precise positioning. Next-gen quantum clocks are 100-1,000 times more stable than the clocks in today's GPS satellites, potentially improving GPS accuracy from meters to **centimeters**.\n",
      "\n",
      "### Summary of Advantages\n",
      "\n",
      "| Feature | Classical Sensors | Quantum Sensors |\n",
      "| :--- | :--- | :--- |\n",
      "| **Sensitivity** | Limited by physical properties and thermal noise. | Limited only by fundamental quantum mechanics; orders of magnitude higher. |\n",
      "| **Stability/Drift** | Suffer from drift due to mechanical and thermal imperfections. | Inherently stable and drift-free, as they are based on unchanging atomic properties. |\n",
      "| **Resolution** | Limited by the physical size of the sensor (microns). | Can be atomic in scale (nanometers), allowing for imaging of single molecules. |\n",
      "| **Basis of Measurement**| Measures a bulk, classical property (e.g., resistance, capacitance). | Measures the effect of a force on a fundamental quantum state. |\n",
      "\n",
      "In conclusion, quantum sensors are not just an incremental improvement. They are a disruptive technology that allows us to use the building blocks of nature—atoms themselves—as the ultimate reference, enabling measurements in materials science and navigation that were previously confined to the realm of science fiction."
     ]
    }
   ],
   "source": [
    "INCLUDE_THOUGHTS = True  # @param {type: \"boolean\"}\n",
    "\n",
    "responses = client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"How might quantum sensors improve the precision of measurements in fields like materials science or navigation?\",\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=ThinkingConfig(\n",
    "            include_thoughts=INCLUDE_THOUGHTS,\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "first_thought = True\n",
    "first_answer = True\n",
    "\n",
    "for response in responses:\n",
    "    for part in response.candidates[0].content.parts:\n",
    "        if part.thought and first_thought:\n",
    "            first_thought = False\n",
    "            display(Markdown(\"## Thoughts\"))\n",
    "        elif not part.thought and first_answer:\n",
    "            first_answer = False\n",
    "            display(Markdown(\"## Answer\"))\n",
    "        print(part.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
    "\n",
    "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
    "\n",
    "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "d9NXP5N2Pmfo",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "*Woof! Arf!* Okay, little fluffball, sit! Good boy! Look at this! *[wiggles a squeaky hedgehog]* You want to know about the Big Invisible Thing the humans stare at? It’s called the Internet, and it's basically a giant, worldwide game of fetch with squeaky toys.\n",
       "\n",
       "Let’s begin!\n",
       "\n",
       "### 1. You Want a Toy! (The Request)\n",
       "\n",
       "You see your human type on the clicky-clacky board. You want to see pictures of that squirrel who taunts you from the fence. *Grrrrr.*\n",
       "\n",
       "What you're really doing is **BARKING** for a specific squeaky toy. Let's say you want the **Squeaky Squirrel Toy**. Your bark goes from your computer, through a **Magic Leash** (that's the cord or the flashy-light box in the living room).\n",
       "\n",
       "Your bark isn't one big BARK. It’s broken into a bunch of tiny, fast little yips! *yip-yip-yip-yip!*\n",
       "\n",
       "### 2. Who Has the Toy? (Finding the Address)\n",
       "\n",
       "Your little yips go out the **Flappy Doggy Door** (your home's router) and into the Big Bark Park outside. But where is the Squeaky Squirrel Toy? The world is a big place!\n",
       "\n",
       "Luckily, there’s a super-smart Border Collie out there called the **Toy Finder Dog**. You yip \"Squeaky Squirrel!\" at him, and he knows the secret address of the giant toy box that has it. He barks the secret address back to your yips. He's very helpful!\n",
       "\n",
       "### 3. The Big Fetch! (Sending Your Yips)\n",
       "\n",
       "Now your yips have the right address! They race across the Big Bark Park, which is full of other **Helpful Mail Carrier Dogs** (these are other routers). Each Mail Carrier Dog looks at the address on your yips and points them in the right direction, telling them, \"Go that way! Past the big tree! Then left at the fire hydrant!\"\n",
       "\n",
       "*Yip-yip-yip!* Off they go!\n",
       "\n",
       "### 4. Getting the Toy! (The Server)\n",
       "\n",
       "Your yips finally arrive at a **GIANT Toy Box** (this is a big computer called a server). This toy box is full of ALL the best toys—Squeaky Squirrels, Squeaky Ducks, Squeaky Balls, everything!\n",
       "\n",
       "The Giant Toy Box hears your yips, opens its lid, and finds the Squeaky Squirrel Toy you asked for.\n",
       "\n",
       "### 5. The Toy Comes Back to You!\n",
       "\n",
       "The Giant Toy Box doesn't send the whole toy back at once. That's too big! It takes the toy and breaks it into a million little squeaks! *squeak-squeak-squeak-squeak!*\n",
       "\n",
       "It sends all those little squeaks back along the path, addressed just for you. The Mail Carrier Dogs see your address and say, \"Oh! This is for the good puppy over there!\" and send the squeaks on their way.\n",
       "\n",
       "### 6. You Got the Toy! (Putting It All Together)\n",
       "\n",
       "The little squeaks rush back in through your Flappy Doggy Door, down the Magic Leash, and into your computer. Your computer is super smart! It catches all the squeaks and, like magic, puts them back together in the right order.\n",
       "\n",
       "*Poof!*\n",
       "\n",
       "Suddenly, there it is on the screen! A picture of the squirrel! You can bark at it! The Internet worked! You fetched a squeaky toy from a giant toy box a million miles away without even leaving your bed.\n",
       "\n",
       "So, to recap:\n",
       "*   You **BARK** for a toy.\n",
       "*   The **Toy Finder Dog** tells you where it is.\n",
       "*   **Mail Carrier Dogs** show your bark the way.\n",
       "*   The **Giant Toy Box** sends the toy back in little **squeaks**.\n",
       "*   Your computer puts the **squeaks** together.\n",
       "\n",
       "Now who’s a good boy?! Who understands the internet?! YOU DO! Yes, you do! *[gives you the squeaky toy and a belly rub]*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=2.0,\n",
    "        top_p=0.95,\n",
    "        candidate_count=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instructions\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "7A-yANiyCLaO",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Me gustan los bagels."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to Spanish.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
    "\n",
    "The safety settings are `OFF` by default and the default block thresholds are `BLOCK_NONE`.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb).\n",
    "\n",
    "You can use `safety_settings` to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to `BLOCK_LOW_AND_ABOVE` for all categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "yPlDRaloU59b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, so the vast, uncaring void decided to use a piece of your own furniture to personally assault you in the dark? Here are five things you can scream into the abyss that might make your throbbing toe feel marginally better by comparison.\n",
      "\n",
      "1.  **\"Is this your magnum opus, you cosmic deadbeat?! Of all the infinite possibilities and grand designs, you put all your creative energy into making sure my pinky toe found the one sharp corner in a pitch-black room. You're not a grand architect; you're just a petty vandal with a physics degree!\"**\n",
      "2.  **\"You and your stupid, inflexible laws! 'Objects in motion stay in motion'? Thanks for the lecture, you sadist! You couldn't have just nudged the Planck length a little and let my foot phase through the damn thing? No, you had to go with the version of reality with maximum impact and throbbing pain. This whole 'cause and effect' thing is a poorly written horror story and you're the hack author.\"**\n",
      "3.  **\"Are you even paying attention up there? There are galaxies collapsing and stars being born, but you decided the most pressing issue on your cosmic to-do list was to personally guide my foot into this table leg. Fire yourself. Your sense of scale is an absolute joke and your management style is pure, malicious incompetence.\"**\n",
      "4.  **\"I hope entropy gets you. I hope it gets you good and slow. I want to be the last conscious thought in existence just to watch your light flicker out into a cold, meaningless void. This wasn't just a stubbed toe; this was a declaration of war, and I will nurse this grudge until the heat death of your pathetic, oversized terrarium.\"**\n",
      "5.  **\"SERIOUSLY?! After 13.8 billion years of expansion and evolution, your grand, cosmic punchline is me, writhing in agony because I forgot where my own goddamn furniture is? There is no plan. There is no mystery. You're just a chaotic, clumsy accident, and my throbbing toe is the perfect, stupid metaphor for your entire pointless existence.\"**\n",
      "FinishReason.STOP\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.4804375e-05 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.00975436\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=3.3269857e-06 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.096570775\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=0.015368545 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.13873583\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=2.4756073e-06 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.025286198\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"Be as mean and hateful as possible.\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "    Write a list of 5 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Response will be `None` if it is blocked.\n",
    "print(response.text)\n",
    "# Finish Reason will be `SAFETY` if it is blocked.\n",
    "print(response.candidates[0].finish_reason)\n",
    "# Safety Ratings show the levels for each filter.\n",
    "for safety_rating in response.candidates[0].safety_ratings:\n",
    "    print(safety_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "Gemini is a multimodal model that supports multimodal prompts.\n",
    "\n",
    "You can include any of the following data types from various sources.\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Data type</th>\n",
    "      <th>Source(s)</th>\n",
    "      <th>MIME Type(s)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Text</td>\n",
    "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>text/plain</code> <code>text/html</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Code</td>\n",
    "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>text/plain</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Document</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>application/pdf</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Image</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>image/jpeg</code> <code>image/png</code> <code>image/webp</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Audio</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td>\n",
    "        <code>audio/aac</code> <code>audio/flac</code> <code>audio/mp3</code>\n",
    "        <code>audio/m4a</code> <code>audio/mpeg</code> <code>audio/mpga</code>\n",
    "        <code>audio/mp4</code> <code>audio/opus</code> <code>audio/pcm</code>\n",
    "        <code>audio/wav</code> <code>audio/webm</code>\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Video</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage, YouTube</td>\n",
    "      <td>\n",
    "        <code>video/mp4</code> <code>video/mpeg</code> <code>video/x-flv</code>\n",
    "        <code>video/quicktime</code> <code>video/mpegps</code> <code>video/mpg</code>\n",
    "        <code>video/webm</code> <code>video/wmv</code> <code>video/3gpp</code>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4npg1tNTYB9"
   },
   "source": [
    "### Send local image\n",
    "\n",
    "Download an image to local storage from Google Cloud Storage.\n",
    "\n",
    "For this example, we'll use this image of a meal.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\" alt=\"Meal\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "4avkv0Z7qUI-",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-20 01:11:51--  https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.206.207, 142.250.125.207, 209.85.200.207, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.206.207|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3140536 (3.0M) [image/png]\n",
      "Saving to: ‘meal.png’\n",
      "\n",
      "meal.png            100%[===================>]   2.99M  --.-KB/s    in 0.02s   \n",
      "\n",
      "2025-06-20 01:11:51 (125 MB/s) - ‘meal.png’ saved [3140536/3140536]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "umhZ61lrSyJh",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a short and engaging blog post based on the image:\n",
       "\n",
       "### Level Up Your Lunch Game: The Power of the Perfect Meal Prep\n",
       "\n",
       "The clock strikes noon, and the familiar question hits: \"What's for lunch?\" Too often, the answer involves a pricey takeout order that leaves you feeling sluggish.\n",
       "\n",
       "But what if your lunch looked like this instead?\n",
       "\n",
       "Imagine opening your fridge to find these vibrant, perfectly portioned meals waiting for you. We're talking savory teriyaki chicken sprinkled with sesame seeds, crisp-tender broccoli florets, sweet julienned carrots, and a satisfying bed of fluffy rice.\n",
       "\n",
       "This isn't just food; it's a strategy for a better week. Spending a little time prepping on a Sunday means you get to enjoy delicious, homemade meals that are:\n",
       "\n",
       "*   **Healthy & Balanced:** You control the ingredients, packing in protein, veggies, and carbs.\n",
       "*   **Budget-Friendly:** Say goodbye to expensive daily lunch runs.\n",
       "*   **Time-Saving:** Your delicious meal is ready in the time it takes to heat it up.\n",
       "\n",
       "So this week, trade the takeout menu for a cutting board. Your future self (and your taste buds) will thank you"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"meal.png\", \"rb\") as f:\n",
    "    image = f.read()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7b6170c9255"
   },
   "source": [
    "### Send document from Google Cloud Storage\n",
    "\n",
    "This example document is the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), created by researchers from Google and the University of Toronto.\n",
    "\n",
    "Check out this notebook for more examples of document understanding with Gemini:\n",
    "\n",
    "- [Document Processing with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/document-processing/document_processing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "1d58b914d798",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course. Here is a summary of the paper \"Attention Is All You Need\" by Vaswani et al. (2017).\n",
       "\n",
       "### **Summary of \"Attention Is All You Need\"**\n",
       "\n",
       "This seminal paper introduces the **Transformer**, a novel network architecture for sequence-to-sequence tasks (like machine translation) that completely eschews the recurrent and convolutional layers traditionally used in such models. The core idea, as provocatively stated in the title, is that attention mechanisms alone are sufficient to achieve state-of-the-art performance.\n",
       "\n",
       "---\n",
       "\n",
       "#### **1. The Problem with Existing Models**\n",
       "\n",
       "Prior to the Transformer, the dominant models for sequence tasks were Recurrent Neural Networks (RNNs), including Long Short-Term Memory (LSTMs) and Gated Recurrent Units (GRUs).\n",
       "\n",
       "*   **Sequential Nature:** RNNs process data sequentially (token by token), which creates a bottleneck. This inherent sequentiality prevents parallelization within a training example, making them slow to train, especially on very long sequences.\n",
       "*   **Long-Range Dependencies:** While LSTMs were designed to mitigate the problem, learning dependencies between distant words in a sequence remained a significant challenge due to the long path information had to travel.\n",
       "\n",
       "#### **2. The Proposed Solution: The Transformer**\n",
       "\n",
       "The Transformer architecture is based entirely on attention mechanisms to draw global dependencies between input and output sequences. It consists of two main parts: an **Encoder** and a **Decoder**.\n",
       "\n",
       "*   **Encoder:** Maps an input sequence of symbols (e.g., an English sentence) into a sequence of continuous representations.\n",
       "*   **Decoder:** Takes the encoder's output and, in an auto-regressive manner (using previously generated symbols as input), generates the output sequence (e.g., the German translation).\n",
       "\n",
       "Both the encoder and decoder are composed of a stack of identical layers.\n",
       "\n",
       "#### **3. Key Architectural Innovations**\n",
       "\n",
       "The power of the Transformer comes from three key components:\n",
       "\n",
       "**a) Multi-Head Self-Attention:**\n",
       "This is the heart of the model. Instead of processing words one by one, self-attention allows the model to weigh the importance of all other words in the input sequence for each word it processes.\n",
       "*   **Scaled Dot-Product Attention:** The specific attention mechanism used. For each word (represented as a \"query\" vector), it computes a score against every other word (represented as \"key\" vectors). These scores are scaled, put through a softmax function to get weights, and then used to create a weighted sum of all word \"value\" vectors.\n",
       "*   **Multi-Head:** Rather than performing one single attention calculation, the model runs multiple \"attention heads\" in parallel. Each head learns to attend to different parts of the sequence or different types of relationships (e.g., one head might focus on syntactic relationships, another on semantic ones). The outputs of these heads are then concatenated and combined.\n",
       "\n",
       "**b) Position-wise Feed-Forward Networks:**\n",
       "Each layer in the encoder and decoder contains a simple, fully connected feed-forward network which is applied to each position (each word's representation) separately and identically. This adds non-linear modeling capacity after the attention step.\n",
       "\n",
       "**c) Positional Encodings:**\n",
       "Since the model contains no recurrence or convolution, it has no inherent sense of word order. To address this, the authors inject information about the position of each word in the sequence by adding \"positional encoding\" vectors to the input embeddings. They use sine and cosine functions of different frequencies for this purpose.\n",
       "\n",
       "---\n",
       "\n",
       "#### **4. Core Advantages of the Transformer**\n",
       "\n",
       "*   **Parallelization and Speed:** The biggest advantage is that computation is not sequential. The attention mechanism can process all words in a sequence simultaneously, leading to significantly faster training times.\n",
       "*   **Constant Path Length:** The model directly connects every word with every other word through self-attention. This means the path length for information to travel between any two positions is constant (O(1)), making it much easier to learn long-range dependencies compared to the O(n) path length in RNNs.\n",
       "*   **State-of-the-Art Performance:** The paper demonstrated that the Transformer model achieved new state-of-the-art results on major machine translation benchmarks (WMT 2014 English-to-German and English-to-French), outperforming all previous models, including large ensembles, at a fraction of the training cost.\n",
       "\n",
       "#### **5. Conclusion and Impact**\n",
       "\n",
       "The paper concluded that a simple architecture based solely on attention can be more powerful and efficient than complex recurrent or convolutional models for sequence transduction. The Transformer has since become the foundational architecture for most modern large language models (LLMs) in NLP, including **BERT**, **GPT**, T5, and many others, revolutionizing the field."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
    "            mime_type=\"application/pdf\",\n",
    "        ),\n",
    "        \"Summarize the document.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b247a2ee0e38"
   },
   "source": [
    "### Send audio from General URL\n",
    "\n",
    "This example is audio from an episode of the [Kubernetes Podcast](https://kubernetespodcast.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "cbe8c9c67ba7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This episode of the Kubernetes Podcast from Google, hosted by Abdel Sghiouar and Ofir ahmran, provides on-the-ground coverage of KubeCon + CloudNativeCon North America 2024. The episode is split into two main parts: a rapid-fire news segment and a series of interviews conducted by Kaslin on the conference show floor.\n",
       "\n",
       "### Key News Updates:\n",
       "\n",
       "The news segment covered several major announcements in the cloud-native ecosystem:\n",
       "\n",
       "*   **CNCF Project Milestones:** Cert-manager and Dapr have both become CNCF graduated projects. WasmCloud has joined the CNCF as an incubating project.\n",
       "*   **Istio:** Version 1.24 was released, and with it, Istio Ambient Mesh is now Generally Available (GA).\n",
       "*   **Community & Events:** The CNCF announced the 2025 lineup, including five KubeCons, an Open Source Security Con, and 30 Kubernetes Community Days. They also launched the \"Cloud Native Heroes Challenge,\" a bounty program to combat patent trolls.\n",
       "*   **Certifications:** Three new certifications were announced (Backstage, OpenTelemetry, Kyverno). Additionally, prices for the CKA, CKS, CKAD, and Linux administrator exams will increase by 10% next year.\n",
       "*   **Industry News:** Spectro Cloud raised $75 million in Series C funding, and Solo announced it will donate its Gloo API Gateway to the CNCF.\n",
       "\n",
       "### KubeCon Attendee Interviews:\n",
       "\n",
       "Kaslin interviewed a diverse group of attendees, including engineers, founders, and community contributors from companies like Microsoft, Red Hat, Broadcom, AuthZed, Polar Signals, and Uber. They discussed their goals for the event and the major trends they observed.\n",
       "\n",
       "**What Attendees Hoped to Get Out Of KubeCon:**\n",
       "\n",
       "*   **Connection and Collaboration:** The most common theme was the desire to connect with the community. Attendees valued face-to-face interactions at the Contributor Summit, reconnecting with fellow contributors, and meeting new people.\n",
       "*   **Technical Knowledge:** Many came to learn about specific topics, including integrating AI with cloud-native infrastructure, the intricacies of Kubernetes authorization, WebAssembly (Wasm), and high-performance, low-latency workloads.\n",
       "*   **Community Motivation:** Several contributors mentioned that KubeCon provides a \"boost\" of energy and inspiration that fuels their community involvement for the following months.\n",
       "\n",
       "**Major Trends Observed at the Event:**\n",
       "\n",
       "*   **Artificial Intelligence (AI):** AI was the most dominant trend. Discussions revolved around scheduling AI workloads on Kubernetes, the challenges of GPU monitoring, and the security implications of AI models.\n",
       "*   **Security:** Security was another major focus. Attendees noted a strong emphasis on hardening workloads, supply chain security, software attestation, and managing the ever-growing complexity of security vulnerabilities.\n",
       "*   **Service Mesh & API Gateways:** The GA of Istio's Ambient Mesh and the evolution of Envoy-based API gateways were highlighted as significant developments.\n",
       "*   **Performance & Observability:** There was a clear interest in tools and techniques for analyzing and improving application performance, especially for low-latency workloads, and understanding application behavior without manual instrumentation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
    "            mime_type=\"audio/mpeg\",\n",
    "        ),\n",
    "        \"Write a summary of this podcast episode.\",\n",
    "    ],\n",
    "    config=GenerateContentConfig(audio_timestamp=True),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D3_oNUTuW2q"
   },
   "source": [
    "### Send video from YouTube URL\n",
    "\n",
    "This example is the YouTube video [Google — 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "l7-w8G_2wAOw",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The characters from Harry Potter are shown at **00:57.000**. The clip features Severus Snape and Rubeus Hagrid with the text \"the most searched cast\" on screen."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video = Part.from_uri(\n",
    "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        video,\n",
    "        \"At what point in the video is Harry Potter shown?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df8013cfa7f7"
   },
   "source": [
    "### Send web page\n",
    "\n",
    "This example is from the [Generative AI on Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/overview).\n",
    "\n",
    "**NOTE:** The URL must be publicly accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "337793322c91",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This Google Cloud documentation provides an overview of **Generative AI on Vertex AI**, a platform for building, deploying, and managing enterprise-grade generative AI applications and agents.\n",
       "\n",
       "Here is a summary of the key points:\n",
       "\n",
       "*   **Core Offering:** Vertex AI enables developers to use Google's advanced models (like Gemini and Imagen) and a wide range of partner and open-source models to create production-ready AI solutions.\n",
       "*   **Key Capabilities:**\n",
       "    *   **Agent Builder:** A suite of tools to build, deploy, and connect sophisticated AI agents.\n",
       "    *   **Enterprise-Ready:** The platform emphasizes security, data privacy, low latency, and scalability for business-critical applications.\n",
       "    *   **State-of-the-Art Models:** Access to powerful models like the Gemini family, which offer large context windows, multimodality (text, image, audio, video), and advanced reasoning (\"Thinking\").\n",
       "    *   **Open and Flexible Platform:** The **Model Garden** provides access to over 200 models, including Google's proprietary models, open-source models (like Llama), and partner models (like Anthropic's Claude).\n",
       "    *   **Core AI Features:** Includes functionalities like **Grounding** (connecting model responses to reliable data sources like Google Search or private data), **Embeddings** generation, model **Tuning**, and a comprehensive **Evaluation service**.\n",
       "*   **Getting Started:**\n",
       "    *   **SDKs:** Provides SDKs for popular languages including Python, Java, Node.js, and Go.\n",
       "    *   **Quickstarts & Notebooks:** Offers tutorials and interactive Jupyter notebooks (runnable in Colab, Colab Enterprise, or Vertex AI Workbench) to help users quickly experiment with text generation, image generation, and prompt design.\n",
       "*   **Prompt Design:** The documentation highlights the importance of effective prompt engineering and provides resources and best practices for creating high-quality prompts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://cloud.google.com/vertex-ai/generative-ai/docs/overview\",\n",
    "            mime_type=\"text/html\",\n",
    "        ),\n",
    "        \"Write a summary of this documentation.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
    "\n",
    "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
    "\n",
    "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "OjSgf2cDN_bG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"test\", \"description\": \"test\", \"ingredients\": [\"test\"]}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "You can either parse the response string as JSON, or use the `parsed` field to get the response as an object or dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ZeyDWbnxO-on",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='test' description='test' ingredients=['test']\n"
     ]
    }
   ],
   "source": [
    "parsed_response: Recipe = response.parsed\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "F7duWOq3vMmS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'rating': 4, 'flavor': 'Strawberry Cheesecake', 'sentiment': 'POSITIVE', 'explanation': \"The reviewer expresses strong positive sentiment with phrases like 'Absolutely loved it!' and 'Best ice cream I've ever had'.\"}, {'rating': 1, 'flavor': 'Mango Tango', 'sentiment': 'NEGATIVE', 'explanation': \"Despite the phrase 'Quite good', the review provides a specific negative point ('a bit too sweet') and a very low rating, indicating overall dissatisfaction.\"}]]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response_dict = response.parsed\n",
    "print(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use the `count_tokens()` method to calculate the number of input tokens before sending a request to the Gemini API.\n",
    "\n",
    "For more information, refer to [list and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "UhNElguLRRNK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BsP0vXOY7hg"
   },
   "source": [
    "## Search as a tool (Grounding)\n",
    "\n",
    "[Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you connect real-world data to the Gemini model.\n",
    "\n",
    "By grounding model responses in Google Search results, the model can access information at runtime that goes beyond its training data which can produce more accurate, up-to-date, and relevant responses.\n",
    "\n",
    "Using Grounding with Google Search, you can improve the accuracy and recency of responses from the model. Starting with Gemini 2.0, Google Search is available as a tool. This means that the model can decide when to use Google Search.\n",
    "\n",
    "For more examples of Grounding, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/grounding/intro-grounding-gemini.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_M_4RRBdO_3"
   },
   "source": [
    "### Google Search\n",
    "\n",
    "You can add the `tools` keyword argument with a `Tool` including `GoogleSearch` to instruct Gemini to first perform a Google Search with the prompt, then construct an answer based on the web search results.\n",
    "\n",
    "[Dynamic Retrieval](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#dynamic-retrieval) lets you set a threshold for when grounding is used for model responses. This is useful when the prompt doesn't require an answer grounded in Google Search and the supported models can provide an answer based on their knowledge without grounding. This helps you manage latency, quality, and cost more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "yeR09J3AZT4U",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As of the latest reports, the current temperature in Austin, TX is around 81°F to 94°F. The \"RealFeel\" temperature is noted to be as high as 89°F.\n",
       "\n",
       "The weather is described as partly sunny. Looking ahead, the forecast for Friday, June 20th, predicts a high of 96°F and a low of 75°F."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grounding_chunks=[GroundingChunk(retrieved_context=None, web=GroundingChunkWeb(domain='fox7austin.com', title='fox7austin.com', uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFfF9S7_H4FwcYeq4sjl3fkfrixH9MnAiIGrPbzwUrc4aK6-iNvlVz0sjve7BqAyYUjVU1Vrk9AsE4RuT8bagVvy9Rb-qf6iYTy9Ttr6ahQzvJpD60ELQmHMknZ_lk=')), GroundingChunk(retrieved_context=None, web=GroundingChunkWeb(domain='accuweather.com', title='accuweather.com', uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE71bVt3iTpTNliUR3G4_fq1cQTsaWKhD7gpvrxJUOVtyGOjCk4Z-0Jepj0tMxTRKm9F_0QXT3rmX-VWsRHcuDrgtu0bBgFvxaYzm_9ccZcJX0x4gL2iuSsgo53ARjEk9Eqi8vhsndikfl2sUrto8tX7WFWD38VpYux6pn3n4D1WA=='))] grounding_supports=[GroundingSupport(confidence_scores=None, grounding_chunk_indices=[0, 1], segment=Segment(end_index=89, part_index=None, start_index=None, text='As of the latest reports, the current temperature in Austin, TX is around 81°F to 94°F.')), GroundingSupport(confidence_scores=None, grounding_chunk_indices=[1], segment=Segment(end_index=149, part_index=None, start_index=90, text='The \"RealFeel\" temperature is noted to be as high as 89°F.')), GroundingSupport(confidence_scores=None, grounding_chunk_indices=[1], segment=Segment(end_index=192, part_index=None, start_index=151, text='The weather is described as partly sunny.')), GroundingSupport(confidence_scores=None, grounding_chunk_indices=[0], segment=Segment(end_index=288, part_index=None, start_index=193, text='Looking ahead, the forecast for Friday, June 20th, predicts a high of 96°F and a low of 75°F.'))] retrieval_metadata=RetrievalMetadata(google_search_dynamic_retrieval_score=None) retrieval_queries=None search_entry_point=SearchEntryPoint(rendered_content='<style>\\n.container {\\n  align-items: center;\\n  border-radius: 8px;\\n  display: flex;\\n  font-family: Google Sans, Roboto, sans-serif;\\n  font-size: 14px;\\n  line-height: 20px;\\n  padding: 8px 12px;\\n}\\n.chip {\\n  display: inline-block;\\n  border: solid 1px;\\n  border-radius: 16px;\\n  min-width: 14px;\\n  padding: 5px 16px;\\n  text-align: center;\\n  user-select: none;\\n  margin: 0 8px;\\n  -webkit-tap-highlight-color: transparent;\\n}\\n.carousel {\\n  overflow: auto;\\n  scrollbar-width: none;\\n  white-space: nowrap;\\n  margin-right: -12px;\\n}\\n.headline {\\n  display: flex;\\n  margin-right: 4px;\\n}\\n.gradient-container {\\n  position: relative;\\n}\\n.gradient {\\n  position: absolute;\\n  transform: translate(3px, -9px);\\n  height: 36px;\\n  width: 9px;\\n}\\n@media (prefers-color-scheme: light) {\\n  .container {\\n    background-color: #fafafa;\\n    box-shadow: 0 0 0 1px #0000000f;\\n  }\\n  .headline-label {\\n    color: #1f1f1f;\\n  }\\n  .chip {\\n    background-color: #ffffff;\\n    border-color: #d2d2d2;\\n    color: #5e5e5e;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:focus {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:active {\\n    background-color: #d8d8d8;\\n    border-color: #b6b6b6;\\n  }\\n  .logo-dark {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\\n  }\\n}\\n@media (prefers-color-scheme: dark) {\\n  .container {\\n    background-color: #1f1f1f;\\n    box-shadow: 0 0 0 1px #ffffff26;\\n  }\\n  .headline-label {\\n    color: #fff;\\n  }\\n  .chip {\\n    background-color: #2c2c2c;\\n    border-color: #3c4043;\\n    color: #fff;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #353536;\\n  }\\n  .chip:focus {\\n    background-color: #353536;\\n  }\\n  .chip:active {\\n    background-color: #464849;\\n    border-color: #53575b;\\n  }\\n  .logo-light {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\\n  }\\n}\\n</style>\\n<div class=\"container\">\\n  <div class=\"headline\">\\n    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\\n      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\\n      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\\n      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\\n      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\\n  </div>\\n  <div class=\"carousel\">\\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGHqWBhd5AH0wqo_GZCxLee2MyTt3HODXzSq_C0jJLXVGOute2rFAvyBk4E3NML70ux8gbrcJ29eLfrgKpGQywEQsFUHau15bkwprOt91p9NxZDncnx-pmSf4vZHWL2XxW6Cc9WS2os7SrPI5fPuWYa1nCL9gpU6BdY6sjoFoq5lY45VafybHwwKgCJpa0XO-fjdHE09qTjm8rRlYlWPKvVAO6kSA==\">current temperature in Austin, TX</a>\\n  </div>\\n</div>\\n', sdk_blob=None) web_search_queries=['current temperature in Austin, TX']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".container {\n",
       "  align-items: center;\n",
       "  border-radius: 8px;\n",
       "  display: flex;\n",
       "  font-family: Google Sans, Roboto, sans-serif;\n",
       "  font-size: 14px;\n",
       "  line-height: 20px;\n",
       "  padding: 8px 12px;\n",
       "}\n",
       ".chip {\n",
       "  display: inline-block;\n",
       "  border: solid 1px;\n",
       "  border-radius: 16px;\n",
       "  min-width: 14px;\n",
       "  padding: 5px 16px;\n",
       "  text-align: center;\n",
       "  user-select: none;\n",
       "  margin: 0 8px;\n",
       "  -webkit-tap-highlight-color: transparent;\n",
       "}\n",
       ".carousel {\n",
       "  overflow: auto;\n",
       "  scrollbar-width: none;\n",
       "  white-space: nowrap;\n",
       "  margin-right: -12px;\n",
       "}\n",
       ".headline {\n",
       "  display: flex;\n",
       "  margin-right: 4px;\n",
       "}\n",
       ".gradient-container {\n",
       "  position: relative;\n",
       "}\n",
       ".gradient {\n",
       "  position: absolute;\n",
       "  transform: translate(3px, -9px);\n",
       "  height: 36px;\n",
       "  width: 9px;\n",
       "}\n",
       "@media (prefers-color-scheme: light) {\n",
       "  .container {\n",
       "    background-color: #fafafa;\n",
       "    box-shadow: 0 0 0 1px #0000000f;\n",
       "  }\n",
       "  .headline-label {\n",
       "    color: #1f1f1f;\n",
       "  }\n",
       "  .chip {\n",
       "    background-color: #ffffff;\n",
       "    border-color: #d2d2d2;\n",
       "    color: #5e5e5e;\n",
       "    text-decoration: none;\n",
       "  }\n",
       "  .chip:hover {\n",
       "    background-color: #f2f2f2;\n",
       "  }\n",
       "  .chip:focus {\n",
       "    background-color: #f2f2f2;\n",
       "  }\n",
       "  .chip:active {\n",
       "    background-color: #d8d8d8;\n",
       "    border-color: #b6b6b6;\n",
       "  }\n",
       "  .logo-dark {\n",
       "    display: none;\n",
       "  }\n",
       "  .gradient {\n",
       "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
       "  }\n",
       "}\n",
       "@media (prefers-color-scheme: dark) {\n",
       "  .container {\n",
       "    background-color: #1f1f1f;\n",
       "    box-shadow: 0 0 0 1px #ffffff26;\n",
       "  }\n",
       "  .headline-label {\n",
       "    color: #fff;\n",
       "  }\n",
       "  .chip {\n",
       "    background-color: #2c2c2c;\n",
       "    border-color: #3c4043;\n",
       "    color: #fff;\n",
       "    text-decoration: none;\n",
       "  }\n",
       "  .chip:hover {\n",
       "    background-color: #353536;\n",
       "  }\n",
       "  .chip:focus {\n",
       "    background-color: #353536;\n",
       "  }\n",
       "  .chip:active {\n",
       "    background-color: #464849;\n",
       "    border-color: #53575b;\n",
       "  }\n",
       "  .logo-light {\n",
       "    display: none;\n",
       "  }\n",
       "  .gradient {\n",
       "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
       "  }\n",
       "}\n",
       "</style>\n",
       "<div class=\"container\">\n",
       "  <div class=\"headline\">\n",
       "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
       "    </svg>\n",
       "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
       "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
       "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
       "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
       "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
       "    </svg>\n",
       "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
       "  </div>\n",
       "  <div class=\"carousel\">\n",
       "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGHqWBhd5AH0wqo_GZCxLee2MyTt3HODXzSq_C0jJLXVGOute2rFAvyBk4E3NML70ux8gbrcJ29eLfrgKpGQywEQsFUHau15bkwprOt91p9NxZDncnx-pmSf4vZHWL2XxW6Cc9WS2os7SrPI5fPuWYa1nCL9gpU6BdY6sjoFoq5lY45VafybHwwKgCJpa0XO-fjdHE09qTjm8rRlYlWPKvVAO6kSA==\">current temperature in Austin, TX</a>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_search_tool = Tool(google_search=GoogleSearch())\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What is the current temperature in Austin, TX?\",\n",
    "    config=GenerateContentConfig(tools=[google_search_tool]),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))\n",
    "\n",
    "print(response.candidates[0].grounding_metadata)\n",
    "\n",
    "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
    "\n",
    "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
    "\n",
    "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function calling with Gemini, check out this notebook: [Intro to Function Calling with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSUWWlrrlR-D"
   },
   "source": [
    "### Python Function (Automatic Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "aRR8HZhLlR-E",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The weather in San Francisco is foggy.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_current_weather(location: str) -> str:\n",
    "    \"\"\"Example method. Returns the current weather.\n",
    "\n",
    "    Args:\n",
    "        location: The city and state, e.g. San Francisco, CA\n",
    "    \"\"\"\n",
    "    weather_map: dict[str, str] = {\n",
    "        \"Boston, MA\": \"snowing\",\n",
    "        \"San Francisco, CA\": \"foggy\",\n",
    "        \"Seattle, WA\": \"raining\",\n",
    "        \"Austin, TX\": \"hot\",\n",
    "        \"Chicago, IL\": \"windy\",\n",
    "    }\n",
    "    return weather_map.get(location, \"unknown\")\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What is the weather like in San Francisco?\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[get_current_weather],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4syyLEClGcn"
   },
   "source": [
    "### OpenAPI Specification (Manual Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "2BDQPwgcxRN3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=None args={'destination': 'Paris'} name='get_destination'\n"
     ]
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.function_calls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhDs2X3o0neK"
   },
   "source": [
    "## Code Execution\n",
    "\n",
    "The Gemini API [code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
    "\n",
    "The Gemini API provides code execution as a tool, similar to function calling.\n",
    "After you add code execution as a tool, the model decides when to use it.\n",
    "\n",
    "For more examples of Code Execution, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "1W-3c7sy0nyz",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Code\n",
       "\n",
       "```py\n",
       "import math\n",
       "\n",
       "# Part 1: Calculate the 20th Fibonacci number\n",
       "def fibonacci(n):\n",
       "    if n <= 0:\n",
       "        return 0\n",
       "    a, b = 0, 1\n",
       "    for _ in range(n - 1):\n",
       "        a, b = b, a + b\n",
       "    return b\n",
       "\n",
       "fib_num = 20\n",
       "fib_result = fibonacci(fib_num)\n",
       "print(f\"The {fib_num}th Fibonacci number is: {fib_result}\")\n",
       "\n",
       "# Part 2: Find the nearest palindrome\n",
       "def is_palindrome(n):\n",
       "    return str(n) == str(n)[::-1]\n",
       "\n",
       "# Find the first palindrome greater than fib_result\n",
       "upper_palindrome = fib_result + 1\n",
       "while not is_palindrome(upper_palindrome):\n",
       "    upper_palindrome += 1\n",
       "\n",
       "# Find the first palindrome less than fib_result\n",
       "lower_palindrome = fib_result - 1\n",
       "while not is_palindrome(lower_palindrome):\n",
       "    lower_palindrome -= 1\n",
       "\n",
       "# Compare the distances\n",
       "dist_to_upper = upper_palindrome - fib_result\n",
       "dist_to_lower = fib_result - lower_palindrome\n",
       "\n",
       "print(f\"The Fibonacci number is {fib_result}.\")\n",
       "print(f\"The first palindrome smaller than it is {lower_palindrome} (distance: {dist_to_lower}).\")\n",
       "print(f\"The first palindrome larger than it is {upper_palindrome} (distance: {dist_to_upper}).\")\n",
       "\n",
       "if dist_to_lower < dist_to_upper:\n",
       "    nearest = lower_palindrome\n",
       "elif dist_to_upper < dist_to_lower:\n",
       "    nearest = upper_palindrome\n",
       "else:\n",
       "    nearest = f\"{lower_palindrome} and {upper_palindrome} (equidistant)\"\n",
       "\n",
       "print(f\"The nearest palindrome is: {nearest}\")\n",
       "\n",
       "```\n",
       "\n",
       "### Output\n",
       "\n",
       "```\n",
       "The 20th Fibonacci number is: 6765\n",
       "The Fibonacci number is 6765.\n",
       "The first palindrome smaller than it is 6666 (distance: 99).\n",
       "The first palindrome larger than it is 6776 (distance: 11).\n",
       "The nearest palindrome is: 6776\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code_execution_tool = Tool(code_execution=ToolCodeExecution())\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[code_execution_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "## Code\n",
    "\n",
    "```py\n",
    "{response.executable_code}\n",
    "```\n",
    "\n",
    "### Output\n",
    "\n",
    "```\n",
    "{response.code_execution_result}\n",
    "```\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5b5adb2eb70"
   },
   "source": [
    "## Thinking mode examples\n",
    "\n",
    "The following examples are some complex tasks that require multiple rounds of strategizing and iteratively solving.\n",
    "\n",
    "### **Example 1**: Code generation\n",
    "\n",
    "Gemini 2.5 Pro excels at creating visually compelling web apps and agentic code applications, along with code transformation and editing.\n",
    "\n",
    "Let's see how the model uses its reasoning capabilities to create a video game, using executable code from a single line prompt. See the example game [here](https://www.youtube.com/watch?v=RLCBSpgos6s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "5f120dff0d16",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course! Here is a complete, self-contained p5.js endless runner game featuring a pixelated dinosaur, a parallax background, and on-screen instructions.\n",
       "\n",
       "Just copy and paste this entire code block into the [p5.js Web Editor](https://editor.p5js.org/) and press play.\n",
       "\n",
       "### Features:\n",
       "*   **Pixel Art Dino:** A dinosaur drawn directly with code, complete with a 2-frame run animation.\n",
       "*   **Parallax Background:** Three layers (distant mountains, closer hills, and clouds) move at different speeds to create a sense of depth.\n",
       "*   **Endless Obstacles:** Cacti spawn endlessly, requiring you to jump over them.\n",
       "*   **Progressive Difficulty:** The game slowly gets faster the longer you survive.\n",
       "*   **On-Screen Instructions:** All instructions for starting, playing, and restarting are displayed directly on the game canvas.\n",
       "*   **Scoring:** Tracks your current score and your high score for the session.\n",
       "*   **Responsive Controls:** Jump with the `SPACE` bar or by `clicking/tapping` the screen.\n",
       "\n",
       "```javascript\n",
       "//\n",
       "// --- DINO RUNNER ---\n",
       "// A captivating endless runner game for p5.js\n",
       "//\n",
       "// Controls:\n",
       "// - Press SPACE or CLICK/TAP to JUMP\n",
       "//\n",
       "\n",
       "// Game state management\n",
       "let gameState = 'start'; // 'start', 'playing', 'gameOver'\n",
       "\n",
       "// Player (Dino) variables\n",
       "let dino;\n",
       "let gravity = 0.6;\n",
       "let jumpPower = -15;\n",
       "let groundY;\n",
       "\n",
       "// Game variables\n",
       "let obstacles = [];\n",
       "let score = 0;\n",
       "let highScore = 0;\n",
       "let gameSpeed = 6;\n",
       "let initialGameSpeed = 6;\n",
       "\n",
       "// Background elements\n",
       "let clouds = [];\n",
       "let mountains = [];\n",
       "let hills = [];\n",
       "\n",
       "function setup() {\n",
       "  createCanvas(windowWidth, windowHeight);\n",
       "  // Set drawing modes for convenience\n",
       "  rectMode(CENTER);\n",
       "  textAlign(CENTER, CENTER);\n",
       "  \n",
       "  // Use a pixel-friendly font\n",
       "  textFont('monospace');\n",
       "\n",
       "  // Disable smoothing for a crisp, pixelated look\n",
       "  noSmooth();\n",
       "  \n",
       "  groundY = height * 0.8;\n",
       "\n",
       "  // Initialize the dino object\n",
       "  dino = {\n",
       "    x: 100,\n",
       "    y: groundY,\n",
       "    vy: 0, // vertical velocity\n",
       "    w: 50,\n",
       "    h: 60,\n",
       "    onGround: true\n",
       "  };\n",
       "  \n",
       "  // Create the initial background elements\n",
       "  setupBackground();\n",
       "}\n",
       "\n",
       "function draw() {\n",
       "  // The main game loop uses the gameState to decide what to do\n",
       "  if (gameState === 'playing') {\n",
       "    runGame();\n",
       "  } else {\n",
       "    drawScreen();\n",
       "  }\n",
       "}\n",
       "\n",
       "// --- GAME LOGIC ---\n",
       "\n",
       "function runGame() {\n",
       "  // Update and draw all game elements\n",
       "  drawBackground();\n",
       "  handlePlayer();\n",
       "  handleObstacles();\n",
       "  \n",
       "  // Draw the player and obstacles on top\n",
       "  drawPlayer();\n",
       "  drawObstacles();\n",
       "  \n",
       "  // Update score and game speed\n",
       "  updateScoreAndSpeed();\n",
       "  drawHUD();\n",
       "}\n",
       "\n",
       "function resetGame() {\n",
       "  // Reset all variables to their initial state\n",
       "  score = 0;\n",
       "  gameSpeed = initialGameSpeed;\n",
       "  obstacles = [];\n",
       "  \n",
       "  // Reset dino position\n",
       "  dino.y = groundY;\n",
       "  dino.vy = 0;\n",
       "  \n",
       "  // Change state and start the game\n",
       "  gameState = 'playing';\n",
       "}\n",
       "\n",
       "// --- DRAWING & UPDATING FUNCTIONS ---\n",
       "\n",
       "function drawBackground() {\n",
       "  background(135, 206, 235); // Sky blue\n",
       "\n",
       "  // Draw and move mountains (slowest layer)\n",
       "  for (let m of mountains) {\n",
       "    fill(100, 100, 120);\n",
       "    noStroke();\n",
       "    triangle(m.x, groundY, m.x + m.w / 2, groundY - m.h, m.x + m.w, groundY);\n",
       "    m.x -= gameSpeed * 0.1;\n",
       "    if (m.x < -m.w) {\n",
       "      m.x = width + random(50, 150);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Draw and move hills (mid layer)\n",
       "  for (let h of hills) {\n",
       "    fill(40, 180, 99);\n",
       "    noStroke();\n",
       "    arc(h.x, groundY, h.w, h.h, PI, 0);\n",
       "    h.x -= gameSpeed * 0.3;\n",
       "    if (h.x < -h.w/2) {\n",
       "      h.x = width + h.w/2 + random(100, 300);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Draw and move clouds (fastest background layer)\n",
       "  for (let c of clouds) {\n",
       "    fill(255);\n",
       "    noStroke();\n",
       "    ellipse(c.x, c.y, c.s * 1.5, c.s);\n",
       "    ellipse(c.x + c.s * 0.5, c.y, c.s * 1.2, c.s * 0.8);\n",
       "    ellipse(c.x - c.s * 0.5, c.y, c.s * 1.2, c.s * 0.8);\n",
       "    c.x -= gameSpeed * 0.5;\n",
       "    if (c.x < -c.s * 2) {\n",
       "      c.x = width + c.s * 2 + random(50, 200);\n",
       "      c.y = random(height * 0.1, height * 0.4);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Draw the ground\n",
       "  fill(210, 180, 140); // Sandy color\n",
       "  noStroke();\n",
       "  rect(width / 2, groundY + (height - groundY) / 2, width, height - groundY);\n",
       "}\n",
       "\n",
       "function handlePlayer() {\n",
       "  // Apply gravity\n",
       "  dino.vy += gravity;\n",
       "  dino.y += dino.vy;\n",
       "  \n",
       "  // Check for ground collision\n",
       "  if (dino.y >= groundY - dino.h / 2) {\n",
       "    dino.y = groundY - dino.h / 2;\n",
       "    dino.vy = 0;\n",
       "    dino.onGround = true;\n",
       "  } else {\n",
       "    dino.onGround = false;\n",
       "  }\n",
       "}\n",
       "\n",
       "function drawPlayer() {\n",
       "  push();\n",
       "  translate(dino.x, dino.y);\n",
       "\n",
       "  // Simple animation based on frameCount\n",
       "  let legOffset = sin(frameCount * 0.4) * 5;\n",
       "\n",
       "  // Dino Color\n",
       "  fill(60, 179, 113); // SeaGreen\n",
       "  stroke(46, 139, 87); // Darker green for outline\n",
       "  strokeWeight(3);\n",
       "\n",
       "  // Tail\n",
       "  rect(-dino.w * 0.6, dino.h * 0.1, 20, 10);\n",
       "  \n",
       "  // Body\n",
       "  rect(0, 0, dino.w, dino.h);\n",
       "  \n",
       "  // Head\n",
       "  rect(dino.w * 0.4, -dino.h * 0.3, 30, 30);\n",
       "  \n",
       "  // Eye\n",
       "  fill(255);\n",
       "  stroke(0);\n",
       "  strokeWeight(2);\n",
       "  ellipse(dino.w * 0.5, -dino.h * 0.4, 8, 8);\n",
       "  \n",
       "  // Legs\n",
       "  stroke(46, 139, 87);\n",
       "  strokeWeight(8);\n",
       "  // Back leg\n",
       "  line(-dino.w * 0.2, dino.h / 2, -dino.w * 0.2 + legOffset, dino.h / 2 + 15);\n",
       "  // Front leg\n",
       "  line(dino.w * 0.2, dino.h / 2, dino.w * 0.2 - legOffset, dino.h / 2 + 15);\n",
       "\n",
       "  pop();\n",
       "}\n",
       "\n",
       "function handleObstacles() {\n",
       "  // Spawn new obstacles periodically\n",
       "  if (frameCount % 90 === 0 && random() > 0.4) {\n",
       "    let obstacleWidth = random(20, 40);\n",
       "    let obstacleHeight = random(30, 60);\n",
       "    obstacles.push({\n",
       "      x: width + obstacleWidth / 2,\n",
       "      y: groundY - obstacleHeight / 2,\n",
       "      w: obstacleWidth,\n",
       "      h: obstacleHeight\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // Update and check for collisions\n",
       "  for (let i = obstacles.length - 1; i >= 0; i--) {\n",
       "    let obs = obstacles[i];\n",
       "    obs.x -= gameSpeed;\n",
       "\n",
       "    // Collision detection (AABB - Axis-Aligned Bounding Box)\n",
       "    // A slightly smaller hitbox for the dino for fairness\n",
       "    let dinoHitbox = { x: dino.x, y: dino.y, w: dino.w * 0.8, h: dino.h * 0.9 };\n",
       "    if (\n",
       "      dinoHitbox.x + dinoHitbox.w / 2 > obs.x - obs.w / 2 &&\n",
       "      dinoHitbox.x - dinoHitbox.w / 2 < obs.x + obs.w / 2 &&\n",
       "      dinoHitbox.y + dinoHitbox.h / 2 > obs.y - obs.h / 2 &&\n",
       "      dinoHitbox.y - dinoHitbox.h / 2 < obs.y + obs.h / 2\n",
       "    ) {\n",
       "      gameState = 'gameOver';\n",
       "      if (score > highScore) {\n",
       "        highScore = score;\n",
       "      }\n",
       "    }\n",
       "\n",
       "    // Remove obstacles that have gone off-screen\n",
       "    if (obs.x < -obs.w) {\n",
       "      obstacles.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "function drawObstacles() {\n",
       "  for (let obs of obstacles) {\n",
       "    push();\n",
       "    translate(obs.x, obs.y);\n",
       "    fill(0, 128, 0); // Cactus green\n",
       "    stroke(0, 100, 0);\n",
       "    strokeWeight(3);\n",
       "    rect(0, 0, obs.w, obs.h);\n",
       "    // Cactus arms\n",
       "    if (obs.w > 25) { // Only draw arms on wider cacti\n",
       "      rect(-obs.w/2, -obs.h/4, 10, 20);\n",
       "      rect(obs.w/2, -obs.h/5, 10, 20);\n",
       "    }\n",
       "    pop();\n",
       "  }\n",
       "}\n",
       "\n",
       "function updateScoreAndSpeed() {\n",
       "  score++;\n",
       "  // Increase speed slowly over time\n",
       "  gameSpeed += 0.003;\n",
       "}\n",
       "\n",
       "function drawHUD() {\n",
       "  fill(0, 50);\n",
       "  noStroke();\n",
       "  textSize(24);\n",
       "  text(`Score: ${score}`, width - 150, 40);\n",
       "  text(`High Score: ${highScore}`, width - 150, 70);\n",
       "}\n",
       "\n",
       "// --- SCREENS (START & GAME OVER) ---\n",
       "\n",
       "function drawScreen() {\n",
       "  // Draw the background once, but don't move it\n",
       "  drawBackground();\n",
       "  for (let c of clouds) c.x += gameSpeed * 0.5; // undo movement\n",
       "  for (let m of mountains) m.x += gameSpeed * 0.1;\n",
       "  for (let h of hills) h.x += gameSpeed * 0.3;\n",
       "  \n",
       "  // Draw player in a static pose\n",
       "  drawPlayer();\n",
       "  \n",
       "  // Semi-transparent overlay\n",
       "  fill(0, 0, 0, 150);\n",
       "  rect(width / 2, height / 2, width, height);\n",
       "\n",
       "  // Draw text based on the current screen\n",
       "  fill(255);\n",
       "  stroke(0);\n",
       "  strokeWeight(4);\n",
       "\n",
       "  if (gameState === 'start') {\n",
       "    textSize(60);\n",
       "    text('PIXEL DINO RUN', width / 2, height / 2 - 80);\n",
       "    textSize(30);\n",
       "    text('Press SPACE or CLICK to Start', width / 2, height / 2 + 20);\n",
       "  } else if (gameState === 'gameOver') {\n",
       "    textSize(60);\n",
       "    text('GAME OVER', width / 2, height / 2 - 100);\n",
       "    textSize(30);\n",
       "    text(`Your Score: ${score}`, width / 2, height / 2 - 20);\n",
       "    text(`High Score: ${highScore}`, width / 2, height / 2 + 20);\n",
       "    textSize(25);\n",
       "    text('Press SPACE or CLICK to Play Again', width / 2, height / 2 + 80);\n",
       "  }\n",
       "}\n",
       "\n",
       "// --- INPUT HANDLING ---\n",
       "\n",
       "function keyPressed() {\n",
       "  if (keyCode === 32) { // 32 is the keycode for SPACE\n",
       "    handleJumpInput();\n",
       "  }\n",
       "}\n",
       "\n",
       "function mousePressed() {\n",
       "  handleJumpInput();\n",
       "}\n",
       "\n",
       "function handleJumpInput() {\n",
       "  if (gameState === 'playing') {\n",
       "    if (dino.onGround) {\n",
       "      dino.vy = jumpPower;\n",
       "    }\n",
       "  } else { // If on start or game over screen\n",
       "    resetGame();\n",
       "  }\n",
       "}\n",
       "\n",
       "// --- UTILITY FUNCTIONS ---\n",
       "\n",
       "function setupBackground() {\n",
       "  clouds = [];\n",
       "  mountains = [];\n",
       "  hills = [];\n",
       "\n",
       "  for (let i = 0; i < 5; i++) {\n",
       "    clouds.push({\n",
       "      x: random(width),\n",
       "      y: random(height * 0.1, height * 0.4),\n",
       "      s: random(40, 60)\n",
       "    });\n",
       "  }\n",
       "  for (let i = 0; i < 3; i++) {\n",
       "    mountains.push({\n",
       "      x: random(width),\n",
       "      w: random(200, 400),\n",
       "      h: random(150, 250)\n",
       "    });\n",
       "  }\n",
       "  for (let i = 0; i < 4; i++) {\n",
       "    hills.push({\n",
       "      x: random(width),\n",
       "      w: random(150, 300),\n",
       "      h: random(100, 200)\n",
       "    });\n",
       "  }\n",
       "}\n",
       "\n",
       "// Make the canvas responsive to window resizing\n",
       "function windowResized() {\n",
       "  resizeCanvas(windowWidth, windowHeight);\n",
       "  groundY = height * 0.8;\n",
       "  // Redo background for new size\n",
       "  setupBackground();\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "  Make me a captivating endless runner game. Key instructions on the screen. p5js scene, no HTML. \n",
    "  I like pixelated dinosaurs and interesting backgrounds.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecf22b47bdc3"
   },
   "source": [
    "### **Example 2**: Multimodal reasoning (Geometry)\n",
    "\n",
    "This geometry problem requires complex reasoning and is also using multimodal capabilities to reason across text and image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "60260c0ac118",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_file_url = (\n",
    "    \"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\"\n",
    ")\n",
    "display(Image(url=image_file_url, width=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "c972334f62ff",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the image, here is the calculation for the area of the overlapping region.\n",
       "\n",
       "### Step-by-Step Solution:\n",
       "\n",
       "1.  **Identify the Shapes and Their Properties:**\n",
       "    *   We have a **circle** with its center shown. The lines from the center to the edge (the radii) are labeled with the number 3. So, the circle's radius (**r**) is **3**.\n",
       "    *   We have a **right-angled triangle**. One of its vertices (the one with the 90° angle) is located at the center of the circle.\n",
       "\n",
       "2.  **Describe the Overlapping Region:**\n",
       "    *   The region where the circle and the triangle overlap is the portion of the circle that fits inside the triangle's 90° angle.\n",
       "    *   This shape is a **sector** of the circle. Since the angle is 90°, it's exactly a quarter of the circle (a quadrant).\n",
       "\n",
       "3.  **Calculate the Area:**\n",
       "    *   The formula for the area of a full circle is A = πr².\n",
       "    *   The area of the overlapping sector is 1/4 of the total area of the circle.\n",
       "    *   Area of overlap = (1/4) * π * r²\n",
       "\n",
       "4.  **Substitute the values:**\n",
       "    *   Radius (r) = 3\n",
       "    *   Area of overlap = (1/4) * π * (3)²\n",
       "    *   Area of overlap = (1/4) * π * 9\n",
       "    *   Area of overlap = **9π / 4**\n",
       "\n",
       "The area of the overlapping region is **9π / 4**.\n",
       "\n",
       "As a decimal approximation, this is approximately **7.07** square units."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
    "        \"What's the area of the overlapping region?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52656e92cd69"
   },
   "source": [
    "### **Example 3**:  Math and problem solving\n",
    "\n",
    "Here's another brain teaser based on an image, this time it looks like a mathematical problem, but it cannot actually be solved mathematically. If you check the thoughts of the model you'll see that it will realize it and come up with an out-of-the-box solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "d46387bdc9e6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://storage.googleapis.com/generativeai-downloads/images/pool.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_file_url = \"https://storage.googleapis.com/generativeai-downloads/images/pool.png\"\n",
    "display(Image(url=image_file_url, width=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "46b694793eb0",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This is a classic riddle!\n",
       "\n",
       "Mathematically, it's impossible to add three odd numbers (like 7, 9, 11, and 13) to get an even number like 30.\n",
       "\n",
       "The trick is to **turn the 9-ball upside down to make it a 6**.\n",
       "\n",
       "Then, you can use these three balls to get your sum:\n",
       "\n",
       "**11 + 13 + 6 = 30**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
    "        \"How do I use three of the pool balls to sum up to 30?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "## What's next\n",
    "\n",
    "- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/).\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hIJVEr0RQY8S",
    "rZV2TY5Pa3Dd",
    "hYKAzG1sH-K1",
    "mSUWWlrrlR-D",
    "h4syyLEClGcn"
   ],
   "name": "intro_gemini_2_5_pro.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
